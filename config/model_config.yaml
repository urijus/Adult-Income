models:
  logistic_regression:
    enabled: true
    parameters:
      penalty: "l2"
      solver: "liblinear"
      max_iter: 100

  decision_tree:
    enabled: true
    parameters:
      criterion: "gini"
      max_depth: 10

  random_forest:
    enabled: true
    parameters:
      n_estimators: 100
      max_depth: 15

  xgboost:
    enabled: true
    parameters:
      learning_rate: 0.1
      n_estimators: 200
      eval_metric: logloss
      max_depth: 6
      gamma: 0.1
      objective: 'binary:logistic'
      min_child_weight: 5
      subsample: 0.8
      colsample_bytree: 0.8
    param_grid:
      learning_rate: [0.01, 0.1, 0.2]
      n_estimators: [50, 100, 200]
      max_depth: [3, 5, 7]
      min_child_weight: [3, 5, 7]
      gamma: [0.2, 0.5, 1]
      subsample: [0.6, 0.8, 1.0]
      colsample_bytree: [0.6, 0.8, 1.0]
      scale_pos_weight: [1, 20, 50]
      reg_alpha: [0, 0.1, 1]
      reg_lambda: [0.1, 1, 10]

  neural_network:
    enabled: true
    parameters:
      hidden_layers: [64, 32]
      activation: "relu"
      dropout_rate: 0.2
      epochs: 10
      batch_size: 32
      learning_rate: 0.001

training:
  test_size: 0.3
  random_state: 42

evaluation:
  metrics: ["accuracy", "precision", "recall", "f1_score"]


